<!doctype html><html lang="en" data-bs-theme="light"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="description" content="Part 25 of 12-Week BusinessMath Series"><meta name="author" content="Justin Purnell"><meta name="generator" content="Ignite v0.2.1"><title>Vector Operations: Foundation for Multivariate Optimization</title><link href="/css/bootstrap.min.css" rel="stylesheet"><link href="/css/prism-default-dark.css" rel="stylesheet"><link href="/css/bootstrap-icons.min.css" rel="stylesheet"><link href="https://www.justinpurnell.com/BusinessMath/week-07/04-thu-vector-operations" rel="canonical"><link href="/feed.rss" rel="alternate" type="application/rss+xml" title="Justin Purnell"><meta property="og:site_name" content="Justin Purnell"><meta property="og:title" content="Vector Operations: Foundation for Multivariate Optimization"><meta property="twitter:title" content="Vector Operations: Foundation for Multivariate Optimization"><meta property="og:description" content="Vector Operations: Foundation for Multivariate Optimization"><meta name="twitter:description" content="Vector Operations: Foundation for Multivariate Optimization"><meta property="og:url" content="https://www.justinpurnell.com/BusinessMath/week-07/04-thu-vector-operations"><meta name="twitter:domain" content="justinpurnell.com"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:dnt" content="on"><meta name="description" content="**Entrepreneurial Strategist & Builder:** Justin Purnell is the Founder of **Ledge Partners**, dedicated to acquiring and leading profitable businesses. His foundation is built on seven years at **Goldman Sachs** as a high yield credit analyst, where he published over **400 research reports** and advised C-level executives.
**Product Innovation and Scale:** He led the complete **re-platforming of technology supporting over 70 e-commerce sites** globally as Head of Product at **Hotels at Home**. He actively builds technical solutions, including pioneering **AI-powered software** to automate product data generation and designing a Swift middleware prototype that cut vendor onboarding time from **four months to two weeks**.
**Driving Financial and Organizational Results:** At **NBCUniversal's Seeso**, he directed product operations and retention initiatives that resulted in a **40% increase in customer Lifetime Value**. He is adept at mobilizing cross-functional teams, having successfully stepped in to **referee a dispute** between NBC and a third-party vendor to foster consensus and launch the Content Commerce platform.
**Creative and Governance Leadership:** Justin combines corporate rigor with creative experience, having directed the Webby-awarded web series **Smart Girls at the Party** and serving as a founding producer for **UCBcomedy.com**. He maintains significant governance roles, including serving as **President of the Princeton University Class of 2000** (1,143 alumni) and managing its annual giving campaigns."><meta name="fediverse:creator" content="@jpurnell@mastodon.social"><meta name="tags" content="Product Manager, Strategy, Product Leader, Goldman Sachs, NBCUniversal, Hotels at Home, AI, LLM, Digital Transformation, e-commerce, Swift, Consensus Leadership, Leadership, Agile, DevOps, Continuous Integration, Continuous Deployment, Cloud, Data Science, Machine Learning, Python, JavaScript, Front-end, Back-end, Full-stack, Responsive Design, Accessibility, SEO, Content Strategy, User Experience, User Interface, Design, Agile, Product Strategy, Technical Strategy"><script>	(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
	new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
	j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
	'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
	})(window,document,'script','dataLayer','GTM-K3TZS8J');</script><link href="/css/main.css" rel="stylesheet"></head><body><div class="col-sm-10 mx-auto"><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-K3TZS8J"
	height="0" width="0" style="display:none;visibility:hidden"></iframe>
<header><nav class="noPrint navbar navbar-expand-md" style="border-bottom: 0.01em solid #d5d5d5;"><div class="container-fluid col"><button type="button" class="navbar-toggler btn" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div id="navbarCollapse" class="collapse navbar-collapse"><ul class="navbar-nav mb-2 mb-md-0 col"><li class="nav-item"><a href="/" class="nav-link">Home</a></li><li class="nav-item"><a href="/about" class="nav-link">About</a></li><li class="nav-item"><a href="/cv" class="nav-link">CV</a></li><li class="nav-item"><a href="/business-math" class="nav-link">BusinessMath</a></li><li class="nav-item"><a href="/next" class="nav-link">NeXT</a></li></ul></div></div></nav></header><div class="row"><div class="col"><h1 class="mainTitle">Vector Operations: Foundation for Multivariate Optimization</h1><div style="margin-bottom: 2em"><p class="blogDateTime" style="margin-left: 1em">BusinessMath Quarterly Series</p><p class="blogDateTime" style="margin-left: 1em">14 min read</p></div><p class="mx-auto blurb" style="width: 70%; max-width: 800px"><p><strong>Part 25 of 12-Week BusinessMath Series</strong></p><hr /><h2>What You‚Äôll Learn</h2><ul><li>Understanding the <code>VectorSpace</code> protocol and why it matters</li><li>Working with Vector2D, Vector3D, and VectorN types</li><li>Performing vector operations: norms, dot products, projections</li><li>Using generic algorithms that work across all vector types</li><li>Creating type-safe multivariate constraints</li><li>Building portfolio weights and feature vectors</li></ul><hr /><h2>The Problem</h2><p>Multivariate optimization requires working with vectors of different dimensions:</p><ul><li><strong>Portfolio optimization</strong>: N-dimensional weights for N assets</li><li><strong>Pricing models</strong>: 2D or 3D parameter spaces</li><li><strong>Machine learning</strong>: High-dimensional feature vectors</li><li><strong>Generic algorithms</strong>: Code that works for any dimension</li></ul><p><strong>Without a unified vector abstraction, you‚Äôd write duplicate code for each dimension (optimize2D, optimize3D, optimizeND, etc.).</strong></p><hr /><h2>The Solution</h2><p>BusinessMath‚Äôs <code>VectorSpace</code> protocol provides a generic interface for vector operations. Write optimization algorithms once, they work for all dimensions.</p><h3>The VectorSpace Protocol</h3><p>A <strong>vector space</strong> is a mathematical structure supporting:</p><ul><li><strong>Vector addition</strong>: v + w</li><li><strong>Scalar multiplication</strong>: Œ± ¬∑ v</li><li><strong>Zero element</strong>: 0</li><li><strong>Norms and distances</strong>: ‚Äñv‚Äñ, ‚Äñv - w‚Äñ</li></ul><p><strong>Protocol Definition</strong> (simplified):</p><pre><code class="language-swift">public protocol VectorSpace: AdditiveArithmetic {
    associatedtype Scalar: Real

    // Required operations
    static var zero: Self { get }
    static func + (lhs: Self, rhs: Self) -> Self
    static func * (lhs: Scalar, rhs: Self) -> Self

    // Norm and distance
    var norm: Scalar { get }
    func dot(_ other: Self) -> Scalar

    // Conversion
    static func fromArray(_ array: [Scalar]) -> Self?
    func toArray() -> [Scalar]
}
</code></pre><p><strong>Why it matters:</strong></p><pre><code class="language-swift">// ‚ùå Before: Duplicate implementations
func optimize2D(_ f: (Vector2D) -> Double, ...) -> Vector2D
func optimize3D(_ f: (Vector3D) -> Double, ...) -> Vector3D
func optimizeND(_ f: (VectorN) -> Double, ...) -> VectorN

// ‚úÖ After: One generic implementation
func optimize<V: VectorSpace>(_ f: (V) -> V.Scalar, ...) -> V
</code></pre><p>One algorithm works for all vector types!</p><hr /><h2>Vector Implementations</h2><p>BusinessMath provides three vector types optimized for different use cases.</p><h3>Vector2D: Fixed 2D Vectors</h3><p><strong>Use Cases:</strong></p><ul><li>Two-variable optimization</li><li>Coordinate systems (x, y)</li><li>Complex numbers (real, imaginary)</li></ul><p><strong>Performance:</strong> Fastest (compile-time optimization, zero array overhead)</p><pre><code class="language-swift">import BusinessMath

// Create a 2D vector
let v = Vector2D<Double>(x: 3.0, y: 4.0)
let w = Vector2D(x: 1.0, y: 2.0)

// Basic operations
let sum = v + w                    // Vector2D(x: 4.0, y: 6.0)
let scaled = 2.0 * v               // Vector2D(x: 6.0, y: 8.0)

// Norm and distance
print(v.norm)                      // 5.0 (‚àö(3¬≤ + 4¬≤))
print(v.distance(to: w))           // 2.828...
print(v.dot(w))                    // 11.0 (3*1 + 4*2)

// 2D-specific operations
print(v.cross(w))                  // 2.0 (pseudo-cross product)
print(v.angle)                     // 0.927... radians (~53¬∞)
let rotated = v.rotated(by: .pi/2) // Vector2D(x: -4.0, y: 3.0)
</code></pre><p><strong>Output:</strong></p><pre><code>5.0
2.8284271247461903
11.0
2.0
0.9272952180016122
Vector2D(x: -4.0, y: 3.0)
</code></pre><hr /><h3>Vector3D: Fixed 3D Vectors</h3><p><strong>Use Cases:</strong></p><ul><li>Three-variable optimization</li><li>3D coordinate systems</li><li>RGB color spaces</li><li>Cross product calculations</li></ul><p><strong>Performance:</strong> Very fast (compile-time optimization)</p><pre><code class="language-swift">import BusinessMath

// Create 3D vectors
let v3 = Vector3D<Double>(x: 1.0, y: 2.0, z: 3.0)
let w3 = Vector3D<Double>(x: 4.0, y: 5.0, z: 6.0)

// Basic operations
let sum3 = v3 + w3                 // Vector3D(x: 5.0, y: 7.0, z: 9.0)
let scaled3 = 2.0 * v3             // Vector3D(x: 2.0, y: 4.0, z: 6.0)

// Norm and dot product
print(v3.norm)                     // 3.742... (‚àö(1¬≤ + 2¬≤ + 3¬≤))
print(v3.dot(w3))                  // 32.0 (1*4 + 2*5 + 3*6)

// 3D-specific: Cross product
let cross = v3.cross(w3)           // Vector3D perpendicular to both
print(cross)                       // Vector3D(x: -3.0, y: 6.0, z: -3.0)

// Verify perpendicularity
print(v3.dot(cross))               // ~0.0 (perpendicular)
print(w3.dot(cross))               // ~0.0 (perpendicular)
</code></pre><p><strong>Output:</strong></p><pre><code>3.7416573867739413
32.0
Vector3D(x: -3.0, y: 6.0, z: -3.0)
0.0
0.0
</code></pre><p><strong>The insight</strong>: Cross product gives a vector perpendicular to both inputs‚Äîuseful for 3D geometry and physics.</p><hr /><h3>VectorN: Variable N-Dimensional Vectors</h3><p><strong>Use Cases:</strong></p><ul><li>High-dimensional optimization (N > 3)</li><li>Portfolio weights (N assets)</li><li>Machine learning feature vectors</li><li>Any variable or runtime-determined dimension</li></ul><p><strong>Performance:</strong> Flexible but has array bounds checking overhead</p><pre><code class="language-swift">import BusinessMath

// Create an N-dimensional vector
let vN = VectorN<Double>([1.0, 2.0, 3.0, 4.0, 5.0])
let wN = VectorN([5.0, 4.0, 3.0, 2.0, 1.0])

// Basic operations
let sumN = vN + wN                 // VectorN([6, 6, 6, 6, 6])
let scaledN = 2.0 * vN             // VectorN([2, 4, 6, 8, 10])

// Norm and dot product
print(vN.norm)                     // 7.416... (‚àö55)
print(vN.dot(wN))                  // 35.0

// Element access
print(vN[0])                       // 1.0
print(vN[2])                       // 3.0

// Statistical operations
print(vN.dimension)                // 5
print(vN.sum)                      // 15.0
print(vN.mean)                     // 3.0
print(vN.standardDeviation())      // 1.581...
print(vN.min)                      // 1.0
print(vN.max)                      // 5.0
</code></pre><p><strong>Output:</strong></p><pre><code>7.416198487095663
35.0
1.0
3.0
5
15.0
3.0
1.5811388300841898
1.0
5.0
</code></pre><hr /><h2>Common Operations</h2><p>All vector types share these operations through the <code>VectorSpace</code> protocol:</p><h3>Arithmetic Operations</h3><pre><code class="language-swift">let v = VectorN([1.0, 2.0, 3.0])
let w = VectorN([4.0, 5.0, 6.0])

// Addition and subtraction
let sum = v + w                    // [5, 7, 9]
let diff = v - w                   // [-3, -3, -3]

// Scalar multiplication
let scaled = 3.0 * v               // [3, 6, 9]
let divided = v / 2.0              // [0.5, 1.0, 1.5]

// Negation
let negated = -v                   // [-1, -2, -3]
</code></pre><hr /><h3>Norms and Distances</h3><pre><code class="language-swift">let v = VectorN([3.0, 4.0])
let w = VectorN([0.0, 0.0])

// Euclidean norm
print(v.norm)                      // 5.0 (‚àö(3¬≤ + 4¬≤))
print(v.squaredNorm)               // 25.0 (faster for comparisons)

// Distance metrics
print(v.distance(to: w))           // 5.0 (Euclidean)
print(v.manhattanDistance(to: w))  // 7.0 (|3| + |4|)
print(v.chebyshevDistance(to: w))  // 4.0 (max(|3|, |4|))
</code></pre><p><strong>Use cases:</strong></p><ul><li><strong>Euclidean</strong>: Standard distance (geometric)</li><li><strong>Manhattan</strong>: City-block distance (grids, taxi routes)</li><li><strong>Chebyshev</strong>: Chessboard distance (king moves)</li></ul><hr /><h3>Dot Products and Angles</h3><pre><code class="language-swift">let v = VectorN([1.0, 0.0, 0.0])
let w = VectorN([0.0, 1.0, 0.0])

// Dot product
print(v.dot(w))                    // 0.0 (perpendicular)

// Cosine similarity
print(v.cosineSimilarity(with: w)) // 0.0 (orthogonal)

// Angle between vectors
let angle = v.angle(with: w)       // œÄ/2 radians (90¬∞)
print(angle * 180 / .pi)           // 90.0 degrees
</code></pre><hr /><h3>Projections</h3><pre><code class="language-swift">let v = VectorN([3.0, 4.0])
let w = VectorN([1.0, 0.0])

// Project v onto w
let projection = v.projection(onto: w)  // [3.0, 0.0]

// Rejection (component perpendicular to w)
let rejection = v.rejection(from: w)    // [0.0, 4.0]

// Verify: v = projection + rejection
print(v == projection + rejection)      // true
</code></pre><p><strong>Application</strong>: Decompose a vector into parallel and perpendicular components.</p><hr /><h3>Normalization</h3><pre><code class="language-swift">let v = VectorN([3.0, 4.0])

// Normalize to unit length
let unit = v.normalized()          // [0.6, 0.8]
print(unit.norm)                   // 1.0

// Verify direction preserved
print(v.cosineSimilarity(with: unit))  // 1.0 (same direction)
</code></pre><p><strong>Use case</strong>: Unit vectors for direction without magnitude.</p><hr /><h2>VectorN-Specific Operations</h2><h3>Construction Methods</h3><pre><code class="language-swift">// From array
let v1 = VectorN([1.0, 2.0, 3.0])

// Repeating value
let v2 = VectorN(repeating: 5.0, count: 10)

// Zero vector
let v3 = VectorN<Double>.zero

// Ones vector
let v4 = VectorN<Double>.ones(dimension: 5)

// Basis vector (one component = 1, rest = 0)
let e2 = VectorN<Double>.basisVector(dimension: 5, index: 2)
// [0, 0, 1, 0, 0]

// Linear space (evenly spaced)
let v5 = VectorN.linearSpace(from: 0.0, to: 10.0, count: 11)
// [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]

// Log space (logarithmically spaced)
let v6 = VectorN.logSpace(from: 1.0, to: 100.0, count: 3)
// [1, 10, 100]
</code></pre><hr /><h3>Functional Operations</h3><pre><code class="language-swift">let v = VectorN([-2.0, -1.0, 0.0, 1.0, 2.0, 3.0])

// Map (element-wise transform)
let squared = v.map { $0 * $0 }    // [4, 1, 0, 1, 4, 9]

// Filter
let positive = v.filter { $0 > 0 } // [1, 2, 3]

// Reduce
let sum = v.reduce(0.0, +)         // 3.0

// Zip with another vector
let w = VectorN([4.0, 5.0, 6.0, 7.0, 8.0, 9.0])
let product = v.zipWith(w, *)      // [-8, -5, 0, 7, 16, 27]
</code></pre><hr /><h2>Real-World Example: Portfolio Weights</h2><pre><code class="language-swift">import BusinessMath

// 4-asset portfolio
let assets = ["US Stocks", "Intl Stocks", "Bonds", "Real Estate"]
let weights = VectorN([0.40, 0.25, 0.25, 0.10])
let expectedReturns = VectorN([0.10, 0.12, 0.04, 0.08])

// Verify fully invested (weights sum to 1.0)
print("Fully invested: \(weights.sum == 1.0)")

// Portfolio expected return (weighted average)
let portfolioReturn = weights.dot(expectedReturns)
print("Portfolio return: \(portfolioReturn.percent(1))")

// Normalize to equal weights for comparison
let equalWeights = VectorN<Double>.equalWeights(dimension: 4)
let equalReturn = equalWeights.dot(expectedReturns)
print("Equal-weight return: \(equalReturn.percent(1))")
</code></pre><p><strong>Output:</strong></p><pre><code>Fully invested: true
Portfolio return: 8.8%
Equal-weight return: 8.5%
</code></pre><hr /><h2>Try It Yourself</h2><details>
<summary>Click to expand full playground code</summary>
<pre><code class="language-swift">import BusinessMath

// Create a 2D vector
let v = Vector2D<Double>(x: 3.0, y: 4.0)
let w = Vector2D(x: 1.0, y: 2.0)

// Basic operations
let sum = v + w                    // Vector2D(x: 4.0, y: 6.0)
let scaled = 2.0 * v               // Vector2D(x: 6.0, y: 8.0)

// Norm and distance
print(v.norm)                      // 5.0 (‚àö(3¬≤ + 4¬≤))
print(v.distance(to: w))           // 2.828...
print(v.dot(w))                    // 11.0 (3*1 + 4*2)

// 2D-specific operations
print(v.cross(w))                  // 2.0 (pseudo-cross product)
print(v.angle)                     // 0.927... radians (~53¬∞)
let rotated = v.rotated(by: .pi/2) // Vector2D(x: -4.0, y: 3.0)
print(rotated.toArray())

// MARK: Vector3D

	// Create 3D vectors
let v_3d = Vector3D<Double>(x: 1.0, y: 2.0, z: 3.0)
let w_3d = Vector3D<Double>(x: 4.0, y: 5.0, z: 6.0)

// Basic operations
let sum3 = v_3d + w_3d                 // Vector3D(x: 5.0, y: 7.0, z: 9.0)
let scaled3 = 2.0 * v_3d             // Vector3D(x: 2.0, y: 4.0, z: 6.0)

// Norm and dot product
print(v_3d.norm)                     // 3.742... (‚àö(1¬≤ + 2¬≤ + 3¬≤))
print(v_3d.dot(w_3d))                  // 32.0 (1*4 + 2*5 + 3*6)

// 3D-specific: Cross product
let cross = v_3d.cross(w_3d)           // Vector3D perpendicular to both
print(cross)                       // Vector3D(x: -3.0, y: 6.0, z: -3.0)

// Verify perpendicularity
print(v_3d.dot(cross))               // ~0.0 (perpendicular)
print(w_3d.dot(cross))               // ~0.0 (perpendicular)

// MARK: VectorN

	// Create an N-dimensional vector
	let vN = VectorN<Double>([1.0, 2.0, 3.0, 4.0, 5.0])
	let wN = VectorN([5.0, 4.0, 3.0, 2.0, 1.0])

	// Basic operations
	let sumN = vN + wN                 // VectorN([6, 6, 6, 6, 6])
	let scaledN = 2.0 * vN             // VectorN([2, 4, 6, 8, 10])

	// Norm and dot product
	print(vN.norm)                     // 7.416... (‚àö55)
	print(vN.dot(wN))                  // 35.0

	// Element access
	print(vN[0])                       // 1.0
	print(vN[2])                       // 3.0

	// Statistical operations
	print(vN.dimension)                // 5
	print(vN.sum)                      // 15.0
	print(vN.mean)                     // 3.0
	print(vN.standardDeviation())      // 1.581...
	print(vN.min)                      // 1.0
	print(vN.max)                      // 5.0

// MARK: - Arithmetic Operations

let v_arith = VectorN([1.0, 2.0, 3.0])
let w_arith = VectorN([4.0, 5.0, 6.0])

// Addition and subtraction
let sum_arith = v_arith + w_arith                    // [5, 7, 9]
let diff_arith = v_arith - w_arith                   // [-3, -3, -3]

// Scalar multiplication
let scaled_arith = 3.0 * v_arith               // [3, 6, 9]
let divided = v_arith / 2.0              // [0.5, 1.0, 1.5]

// Negation
let negated = -v_arith                   // [-1, -2, -3]

// MARK: - Norms and Distances

let v_norm = VectorN([3.0, 4.0])
let w_norm = VectorN([0.0, 0.0])

// Euclidean norm
print(v_norm.norm)                      // 5.0 (‚àö(3¬≤ + 4¬≤))
print(v_norm.squaredNorm)               // 25.0 (faster for comparisons)

// Distance metrics
print(v_norm.distance(to: w_norm))           // 5.0 (Euclidean)
print(v_norm.manhattanDistance(to: w_norm))  // 7.0 (|3| + |4|)
print(v_norm.chebyshevDistance(to: w_norm))  // 4.0 (max(|3|, |4|))


// MARK: - Dot Products and Angles

let v_dot = VectorN([1.0, 0.0, 0.0])
let w_dot = VectorN([0.0, 1.0, 0.0])

// Dot product
print(v_dot.dot(w_dot))                    // 0.0 (perpendicular)

// Cosine similarity
print(v_dot.cosineSimilarity(with: w_dot)) // 0.0 (orthogonal)

// Angle between vectors
let angle_dot = v_dot.angle(with: w_dot)       // œÄ/2 radians (90¬∞)
print(angle_dot * 180 / .pi)           // 90.0 degrees


// MARK: Projections

let v_proj = VectorN([3.0, 4.0])
let w_proj = VectorN([1.0, 0.0])

// Project v onto w
let projection = v_proj.projection(onto: w_proj)  // [3.0, 0.0]

// Rejection (component perpendicular to w)
let rejection = v_proj.rejection(from: w_proj)    // [0.0, 4.0]

// Verify: v = projection + rejection
print(v_proj == projection + rejection)      // true

// MARK: - Normalization



// Normalize to unit length
let unit = v_norm.normalized()          // [0.6, 0.8]
print(unit.norm)                   // 1.0

// Verify direction preserved
print(v_norm.cosineSimilarity(with: unit))  // 1.0 (same direction)


// MARK: - VectorN Specific Construction

	// From array
	let v1 = VectorN([1.0, 2.0, 3.0])

	// Repeating value
	let v2 = VectorN(repeating: 5.0, count: 10)

	// Zero vector
	let v3 = VectorN<Double>.zero

	// Ones vector
	let v4 = VectorN<Double>.ones(dimension: 5)

	// Basis vector (one component = 1, rest = 0)
	let e2 = VectorN<Double>.basisVector(dimension: 5, index: 2)
	// [0, 0, 1, 0, 0]

	// Linear space (evenly spaced)
	let v5 = VectorN.linearSpace(from: 0.0, to: 10.0, count: 11)
	// [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]

	// Log space (logarithmically spaced)
	let v6 = VectorN.logSpace(from: 1.0, to: 100.0, count: 3)
	// [1, 10, 100]

// MARK: - Functional Operations

let v_func = VectorN([-2.0, -1.0, 0.0, 1.0, 2.0, 3.0])

// Map (element-wise transform)
let squared_func = v_func.map { $0 * $0 }    // [4, 1, 0, 1, 4, 9]

// Filter
let positive_func = v_func.filter { $0 > 0 } // [1, 2, 3]

// Reduce
let sum_func = v_func.reduce(0.0, +)         // 3.0

// Zip with another vector
let w_func = VectorN([4.0, 5.0, 6.0, 7.0, 8.0, 9.0])
let product_func = v_func.zipWith(w_func, *)      // [-8, -5, 0, 7, 16, 27]
print(product_func)


// MARK: Portfolio Weights Example

// 4-asset portfolio
let assets = ["US Stocks", "Intl Stocks", "Bonds", "Real Estate"]
let weights = VectorN([0.40, 0.25, 0.25, 0.10])
let expectedReturns = VectorN([0.10, 0.12, 0.04, 0.08])

// Verify fully invested (weights sum to 1.0)
print("Fully invested: \(weights.sum == 1.0)")

// Portfolio expected return (weighted average)
let portfolioReturn = weights.dot(expectedReturns)
print("Portfolio return: \(portfolioReturn.percent(1))")

// Equal weights for comparison (each asset gets 25%)
let equalWeights = VectorN<Double>.equalWeights(dimension: 4)
print("Equal weights: \(equalWeights.toArray())")  // [0.25, 0.25, 0.25, 0.25]
print("Sum: \(equalWeights.sum)")  // 1.0
let equalReturn = equalWeights.dot(expectedReturns)
print("Equal-weight return: \(equalReturn.percent(1))")  // 8.5%

// MARK: - Simplex Projection vs Normalization

// Demonstrate the difference between simplex projection and normalization
let rawScores = VectorN([3.0, 1.0, 2.0])

// Simplex projection: components sum to 1.0
let probabilities = rawScores.simplexProjection()
print("\nSimplex projection (sum = 1.0):")
print("  Values: \(probabilities.toArray().map { $0.number(3) })")
print("  Sum: \(probabilities.sum.number(2))")
print("  Norm: \(probabilities.norm.number(3))")

// Normalization: Euclidean norm = 1.0
let unitVector = rawScores.normalized()
print("\nNormalization (norm = 1.0):")
print("  Values: \(unitVector.toArray().map { $0.number(3) })")
print("  Sum: \(unitVector.sum.number(3))")
print("  Norm: \(unitVector.norm.number(2))")

</code></pre></details>
<p>‚Üí Full API Reference: <a href="https://github.com/jpurnell/BusinessMath/blob/main/Sources/BusinessMath/BusinessMath.docc/5.4-VectorOperations.md">BusinessMath Docs ‚Äì 5.4 Vector Operations</a></p><p><strong>Modifications to try</strong>:</p><ol><li>Build a 10-asset portfolio and compute risk contribution per asset</li><li>Use cross product to compute area of triangle (3D vectors)</li><li>Implement Gram-Schmidt orthogonalization using projections</li><li>Compare performance: Vector2D vs. VectorN for 2D optimization</li></ol><hr /><h2>Real-World Application</h2><ul><li><strong>Portfolio management</strong>: Represent asset allocations as vectors</li><li><strong>Machine learning</strong>: Feature vectors, gradient descent</li><li><strong>Engineering</strong>: Force vectors, velocity vectors, state spaces</li><li><strong>Optimization</strong>: Multivariate parameter spaces</li></ul><p><strong>Data scientist use case</strong>: ‚ÄúI need to optimize hyperparameters for a model with 20 features. The algorithm should work whether I have 2 features or 200.‚Äù</p><p>Generic vector operations make this trivial.</p><hr /><p><code>‚òÖ Insight ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ</code></p><p><strong>Why Dot Product Measures Similarity</strong></p><p>The dot product v ¬∑ w = ‚Äñv‚Äñ ‚Äñw‚Äñ cos(Œ∏) combines magnitude and angle.</p><p><strong>Cosine similarity</strong> normalizes out magnitude: cos(Œ∏) = (v ¬∑ w) / (‚Äñv‚Äñ ‚Äñw‚Äñ)</p><p><strong>Interpretation:</strong></p><ul><li>cos(Œ∏) = 1: Same direction (parallel)</li><li>cos(Œ∏) = 0: Perpendicular (orthogonal)</li><li>cos(Œ∏) = -1: Opposite direction (antiparallel)</li></ul><p><strong>Application - Portfolio correlation:</strong>If returns for two assets are vectors over time, their cosine similarity measures correlation. High similarity means they move together (bad for diversification).</p><p><strong>Rule of thumb:</strong> Maximize portfolio diversity = minimize pairwise cosine similarity.</p><p><code>‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ</code></p><hr /><h3>üìù Development Note</h3><p>The hardest design decision was <strong>choosing the right vector protocol hierarchy</strong>. We considered:</p><ol><li><strong>Single protocol</strong> (what we chose): <code>VectorSpace</code> with all operations</li><li><strong>Layered protocols</strong>: <code>VectorAddition</code>, <code>VectorNorm</code>, <code>VectorDot</code></li><li><strong>Class hierarchy</strong>: <code>AbstractVector</code> base class</li></ol><p><strong>We chose single protocol because:</strong></p><ul><li>Swift favors protocol composition over class inheritance</li><li>All vector operations need all capabilities (no partial implementations)</li><li>Generic constraints are simpler: <code><V: VectorSpace></code> vs. <code><V: VectorAddition & VectorNorm & VectorDot></code></li></ul><p><strong>Trade-off:</strong> Implementing VectorSpace requires all methods. But this ensures every vector type is fully functional.</p><p><strong>Related Methodology</strong>: <a href="../week-01/03-wed-architecture-patterns">Protocol-Oriented Design</a> (Week 1) - Covered protocol composition and generic programming.</p><hr /><h2>Next Steps</h2><p><strong>Coming up next week</strong>: Advanced Optimization (Week 8) - Multivariate Newton-Raphson, constrained optimization with Lagrange multipliers, and a portfolio optimization case study.</p><hr /><p><strong>Series Progress</strong>:</p><ul><li>Week: 7/12</li><li>Posts Published: 25/~48</li><li>Playgrounds: 21 available</li></ul></p><hr /><div style="margin-top: 2em"><p class="blurb" style="font-style: italic">Tagged with: businessmath, swift, vectors, vectorspace, linear-algebra, protocols, generics</p></div></div></div><header><nav class="noPrint navbar navbar-expand-md" style="border-top: 0.01em solid #d5d5d5;"><div class="container-fluid col"><button type="button" class="navbar-toggler btn" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div id="navbarCollapse" class="collapse navbar-collapse"><ul class="navbar-nav mb-2 mb-md-0 col"><li class="nav-item"><a href="mailto:morals.tech.0x@icloud.com" id="email" rel="me" target="_blank" class="nav-link">email</a></li><li class="nav-item"><a href="https://cal.com/jpurnell/15min" id="calendar" rel="me" target="_blank" class="nav-link">calendar</a></li><li class="nav-item"><a href="http://blog.justinpurnell.com" id="blog" rel="me" target="_blank" class="nav-link">blog</a></li><li class="nav-item"><a href="https://github.com/jpurnell" id="github" rel="me" target="_blank" class="nav-link">github</a></li><li class="nav-item"><a href="https://bsky.app/profile/justinpurnell.com" id="bsky" rel="me" target="_blank" class="nav-link">bsky</a></li><li class="nav-item"><a href="https://mastodon.social/@jpurnell" id="mastodon" rel="me" target="_blank" class="nav-link">mastodon</a></li><li class="nav-item"><a href="https://music.apple.com/us/station/justin-purnells-station/ra.u-a475786ae9cc432a1abb70ff757aa95f" id="radio" rel="me" target="_blank" class="nav-link">radio</a></li><li class="nav-item"><a href="https://www.justinpurnell.com/feed.rss" id="rss" rel="me" target="_blank" class="nav-link">rss</a></li><li class="nav-item"><a href="#" id="theme-toggle" rel="me" target="_blank" class="nav-link">theme</a></li></ul></div></div></nav></header><script src="/js/theme-toggle.js"></script></div><script src="/js/bootstrap.bundle.min.js"></script><script src="/js/syntax-highlighting.js"></script></body></html>